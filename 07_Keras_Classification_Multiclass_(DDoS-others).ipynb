{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    **Keras - Multiclass Clasification with Multiple Files categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook \n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Getting the data in a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack      55663\n",
       "Natural     18309\n",
       "NoEvents     4405\n",
       "Name: marker, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"e:/dalhousie_spark/multiple_attacks/3\")  # csv files with binary attacks categorical labeled\n",
    "all_filenames = [i for i in glob.glob('*.csv')]\n",
    "df = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "df['marker'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data cleansing & encode label in column marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "enc = LabelEncoder()\n",
    "df[\"marker\"] = enc.fit_transform(df[\"marker\"])\n",
    "\n",
    "# elimination nan and inf\n",
    "df1 = df.replace([np.inf, -np.inf], np.nan)\n",
    "m = df1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    How many classes do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    55663\n",
       "1    18309\n",
       "2     4405\n",
       "Name: marker, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['marker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# creating input columns and target column\n",
    "X= m.iloc[:,0:127]\n",
    "y= m.iloc[:,128]\n",
    "dummy_y = np_utils.to_categorical(y)\n",
    " #regularizing the dataset (works better)\n",
    " #standardizing  input features\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Define the network for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Sequential()\n",
    "NN.add(Dense(256, activation='relu', kernel_initializer='random_normal', input_dim=127))\n",
    "NN.add(Dense(256, activation='relu', kernel_initializer='random_normal'))\n",
    "NN.add(Dense(3, activation='softmax', kernel_initializer='random_normal'))\n",
    "NN.compile(optimizer ='adam',loss='categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "50451/50451 [==============================] - 85s 2ms/step - loss: 0.6512 - acc: 0.7162\n",
      "Epoch 2/250\n",
      "50451/50451 [==============================] - 71s 1ms/step - loss: 0.5958 - acc: 0.7224\n",
      "Epoch 3/250\n",
      "50451/50451 [==============================] - 70s 1ms/step - loss: 0.5725 - acc: 0.7279\n",
      "Epoch 4/250\n",
      "31870/50451 [=================>............] - ETA: 28s - loss: 0.5607 - acc: 0.7316"
     ]
    }
   ],
   "source": [
    "NN.fit(X_train,to_categorical(y_train), batch_size=10, epochs=250)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "accuracy_score(y_test, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
